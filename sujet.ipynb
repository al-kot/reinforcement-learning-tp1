{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction à l'apprentissage par renforcement\n",
    "# TP 1 - le bandit-manchot multi-bras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition des vaccins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ArmBernoulli(AbstractArm):\n",
    "    def __init__(self, p: float, random_state: int = 0):\n",
    "        \"\"\"\n",
    "        Bernoulli arm\n",
    "        Args:\n",
    "             p (float): mean parameter\n",
    "             random_state (int): seed to make experiments reproducible\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        super(ArmBernoulli, self).__init__(\n",
    "            mean=p, variance=p * (1. - p), random_state=random_state\n",
    "        )\n",
    "\n",
    "    def sample(self):\n",
    "        return self.local_random.rand(1) < self.p\n",
    "\n",
    "    \n",
    "K = 5\n",
    "T = 100\n",
    "np.random.seed(1)\n",
    "means = np.random.random(K)\n",
    "MAB = [ArmBernoulli(m) for m in means]\n",
    "assert(MAB[0].mean == means[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vaccins :** ils sont notés $A_k\\leq K$, leur probabilité d'immunisation est $p_k = \\mu_k$.\n",
    "\n",
    "**Récompense :** 1 si le patient $t$ est immunisé et 0 sinon.\n",
    "\n",
    "**Action optimale :** choisir le vaccin $a$ avec la meilleure probabilité $a^* = \\arg\\max_{a\\leq K} \\mu_{a}$.\n",
    "\n",
    "**Regret :** $R_T =T\\mu_a^* - \\mathbb{E} \\left[ \\sum_{t=1}^T r_t \\right]$\n",
    "En notant $N_a(t)$, le nombre de fois que le vaccin $a$ a été choisi jusqu'à $t$ et $\\Delta_a = \\mu_a^* - \\mu_a$, le regret s'exprime :\n",
    "\n",
    "$$R_T = \\sum_{a=1}^K \\Delta_a \\mathbb{E} \\left[ N_{a_T} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expliquez ce que signifie le regret concrètement. Pourquoi minimiser le regret donne une bonne solution au problème du bandit-manchot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La solution naive consiste en 2 étapes distinctes :\n",
    "\n",
    "1. Entraînement : on teste les K vaccins sur N patients et on note leur efficacité séparemment\n",
    "2. Exploitation : le vaccin qui apparaît comme le meilleur est choisi pour les T-N patients restants.\n",
    "\n",
    "**Implémentez cet algorithme avec T=100, N=20, K=5. Quel bras a été choisi lors de la seconde phase ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtenez-vous toujours le résultat ? Testez l'algorithme avec une valeur de N plus faible. Qu'en concluez-vous ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracez l'évolution du regret $r_t$ de votre algorithme en fonction du résultat obtenu avec le patient $t<T$ sur plusieurs expériences. Représentez l'évolution moyenne de ce regret sur plusieurs expériences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bounds (P. Auer, 2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'action $a_t$ l'instant $t$ est donnée par : \n",
    "\n",
    "$$a_t = max_a \\left[ X_t(a) + B_t(a) \\right]$$\n",
    "\n",
    "où $X_t(a)$ est la moyenne empirique de la récompense obtenue avec le vaccin $a$ et $B_t(a)$ est un biais :\n",
    "\n",
    "$$B_t(a) = \\sqrt{ \\frac{2\\log t}{T_a}}$$ où $T_a$ est le nombre de fois que le vaccin $a$ a été choisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pourquoi cet algorithme n'a pas besoin d'une phase d'entraînement ? Quel terme favorise l'exploration et quel terme favorise l'exploitation ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implémentez l'algorithme et tracez l'évolution du regret à partir de cet algorithme.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Créez une animation avec matplotlib qui permette de visualiser l'évolution des valeurs de $(X_a)_a$ et de $(B_a)_a$ au cours du temps**\n",
    "\n",
    "Ici, c'est à vous de réfléchir à la meilleure représentation de ces valeurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echantillonnage de Thomson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de Thompson calcule à chaque instant $t$ le paramètre des lois Beta pour tous les vaccins. Il échantillonne alors une probabilité de choisir chaque vaccin et choisit le vaccin avec la plus grande probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment fonctionnent les lois Beta ? Comment les mettre à jour selon les résultats des vaccins ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment se passent l'exploration et l'exploitation ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implémentez l'algorithme et tracez l'évolution du regret.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Créez une animation avec matplotlib qui permette de visualiser l'évolution des probabilités au cours du temps**\n",
    "\n",
    "Indice : pour représenter des probabilités, on pourra utiliser des diagrammes en violons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
